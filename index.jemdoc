# jemdoc: menu{MENU}{index.html}, showsource
= Jun-Kun Wang 
#[http://stanford.edu/~jacobm/ Jacob Mattingley] ([jacobm@stanford.edu])

~~~
{}{img_left}{icml_me.png}{}{260}{200}{}

I am an assistant professor in the Department of Electrical and Computer Engineering and the HalicioÄŸlu Data Science Institute
at UC San Diego. My research interests are optimization and machine learning. I love discovering connections between different research areas, e.g., 
[https://link.springer.com/article/10.1007/s10107-023-01976-y  optimization and no-regret learning], [https://arxiv.org/abs/2207.02189 optimization and sampling], [https://arxiv.org/abs/2210.10019 optimization and tackling distribution shifts].
I like answering fundamental questions (e.g.,
[https://arxiv.org/abs/2206.11872 when Heavy Ball has provable acceleration guarantees]) and designing fast algorithms (e.g.,
[https://openreview.net/forum?id=yYbhKqdi7Hz 1], [Accelerating Hamiltonian Monte Carlo via Chebyshev Integration Time 2]).

I was a postoc at Yale University working with [http://www.cs.yale.edu/homes/wibisono/ Professor Andre Wibisono]. I received my CS PhD from Georgia Tech and was very fortunate to be advised by [https://www.cc.gatech.edu/~jabernethy9/  Professor Jacob Abernethy]. I hold my M.S. in Communication Engineering and B.S. in Electrical Engineering from National Taiwan University. 


~~

Email: jkw005 \[at] ucsd  \[dot\] edu


~~~
*Publications:  \*Corresponding Author/\*Presenting Author *

[https://link.springer.com/article/10.1007/s10107-023-01976-y No-Regret Dynamics in the Fenchel Game: A Unified Framework for Algorithmic Convex Optimization.] \n
Jun-Kun Wang, Jacob Abernethy, Kfir Y. Levy. \n
[https://link.springer.com/article/10.1007/s10107-023-01976-y Mathematical Programming 2023] 

[https://arxiv.org/abs/2207.02189 Accelerating Hamiltonian Monte Carlo via Chebyshev Integration Time] \n
Jun-Kun Wang and Andre Wibisono \n
In ICLR /(International Conference on Learning Representations)/, 2023.

[https://openreview.net/forum?id=yYbhKqdi7Hz Continuized Acceleration for Quasar Convex Functions in Non-Convex Optimization] \n
Jun-Kun Wang and Andre Wibisono \n
In ICLR /(International Conference on Learning Representations)/, 2023.

[https://arxiv.org/abs/2210.10019 Towards Understanding GD with Hard and Conjugate Pseudo-labels for Test-Time Adaptation] \n
Jun-Kun Wang and Andre Wibisono \n
In ICLR /(International Conference on Learning Representations)/, 2023.

[https://arxiv.org/abs/2206.11872   Provable Acceleration of Heavy Ball beyond Quadratics for a class of Polyak-Lojasiewicz Functions when the Non-Convexity is Averaged-Out] \n
Jun-Kun Wang, Chi-Heng Lin, Andre Wibisono, and Bin Hu \n
In ICML /(International Conference on Machine Learning)/, 2022.

[https://arxiv.org/abs/2106.12923 Understanding Modern Techniques in Optimization: Frank-Wolfe, Nesterov's Momentum, and Polyak's Momentum.] \n
PhD Dissertation at Georgia Tech. 2021.

[https://arxiv.org/abs/2010.01618 A Modular Analysis of Provable Acceleration via Polyak's momentum: Training a Wide ReLU Network and a Deep Linear Network ] \n
Jun-Kun Wang, Chi-Heng Lin, and Jacob Abernethy. \n 
In ICML /(International Conference on Machine Learning)/, 2021.

[https://arxiv.org/abs/2010.01637 Understanding How Over-Parametrization Leads to Acceleration: A case of learning a single teacher neuron] \n
Jun-Kun Wang and Jacob Abernethy. \n
In ACML /(Asian Conference on Machine Learning)/, 2021.

[https://openreview.net/forum?id=rkeNfp4tPr  Escape Saddle Points Faster with Stochastic Momentum]. \n
Jun-Kun Wang, Chi-Heng Lin, and Jacob Abernethy. \n
In ICLR /(International Conference on Learning Representations)/, 2020.

[http://proceedings.mlr.press/v98/wang19b/wang19b.pdf Online Linear Optimization with Sparsity Constraints] \n
*\**Jun-Kun Wang, *\** Chi-Jen Lu, and Shou-De Lin. \n
In ALT /(International Conference on Algorithmic Learning Theory)/, 2019.

[https://arxiv.org/abs/1811.05831  Revisiting Projection-Free Optimization For Strongly Convex Constraint Sets] \n
Jarrid Rector-Brooks, Jun-Kun Wang, and Barzan Mozafari. \n
In /AAAI 33, 2019/.

[https://arxiv.org/abs/1807.10455 Acceleration through Optimistic No-Regret Dynamics] \n
*\**Jun-Kun Wang and Jacob Abernethy. \n 
In NeurIPS (/Annual Conference on Neural Information Processing Systems/), 2018. 
(Spotlight) [https://arxiv.org/pdf/1807.10455.pdf paper]

[http://proceedings.mlr.press/v75/abernethy18a Faster Rates for Convex-Concave Games] \n
(name order) Jacob Abernethy, Kevin Lai, Kfir Levy, and *\**Jun-Kun Wang. \n
In COLT /(Computational Learning Theory)/, 2018.

[https://papers.nips.cc/paper/2017/hash/7371364b3d72ac9a3ed8638e6f0be2c9-Abstract.html On Frank-Wolfe and Equilibrium Computation]\n
Jacob Abernethy and *\**Jun-Kun Wang. \n
In NeurIPS (/Annual Conference on Neural Information Processing Systems/), 2017. 
(Spotlight)
[./papers/nips_2017.pdf paper] [./papers/nips17_supp.pdf supplementary]

[./papers/dsaa2016_admm.pdf Efficient Sampling-based ADMM for Distributed Data] \n *\**Jun-Kun Wang, Shou-De Lin.\n
In DSAA /(IEEE International Conference on Data Science and Advanced Analytics)/, 2016. [https://github.com/j123456/dis_sdcaadmm code]

[./papers/dsaa2016_plsi.pdf Parallel Least-Squares Policy Iteration] \n
*\**Jun-Kun Wang, Shou-De Lin. \n
In DSAA /(IEEE International Conference on Data Science and Advanced Analytics)/,
2016.

[https://proceedings.mlr.press/v32/wangf14.html Robust Inverse Covariance Estimation under Noisy Measurements] \n *\**Jun-Kun Wang, Shou-De Lin.\n
In ICML /(International Conference on Machine Learning)/, 2014.
[http://jmlr.org/proceedings/papers/v32/wangf14.pdf paper]
[./papers/slide_icml14.pdf slide]
~~~

~~~
* Techical Reports: *

[1] [https://arxiv.org/pdf/2010.01449  Quickly Finding a Benign Region via Heavy Ball Momentum in Non-Convex Optimization ] \n
Jun-Kun Wang, Jacob Abernethy. \n
arXiv 2020.
~~~

~~~
Reviewer of NeurIPS 2016,2017,2018,2019,2020,2021,2022, of ICML 2017,2018,2019,2020,2021,2022, of COLT 2017,2018,2019,2020,2021,2022, of ALT 2017,2018,2019,2020,2022, of ICLR 2021,2022
~~~