# jemdoc: menu{MENU}{index.html}, showsource
= Jun-Kun Wang 
#[http://stanford.edu/~jacobm/ Jacob Mattingley] ([jacobm@stanford.edu])

~~~
{}{img_left}{JW3.jpg}{}{220}{200}{}

I am an assistant professor in the Department of Electrical and Computer Engineering and the HalicioÄŸlu Data Science Institute
at UC San Diego. My research is centered around optimization and
its connections to statistics, machine learning, and classical mechanics. I like discovering connections between different research areas, e.g., 
[https://link.springer.com/article/10.1007/s10107-023-01976-y  optimization and no-regret learning], [https://arxiv.org/abs/2207.02189 optimization and sampling], [https://arxiv.org/abs/2402.13988 optimization and Hamiltonian mechanics].


I was a postdoc at Yale University working with [http://www.cs.yale.edu/homes/wibisono/ Professor Andre Wibisono]. I received my CS PhD from Georgia Tech and was very fortunate to be advised by [https://www.cc.gatech.edu/~jabernethy9/  Professor Jacob Abernethy]. I hold my M.S. in Communication Engineering and B.S. in Electrical Engineering from National Taiwan University. 


Email: jkw005 \[at] ucsd  \[dot\] edu



~~~

~~~

* Teaching: *

Winter 2024 DSC 211 Introduction to Optimization

Spring 2024 [ECE_273.html ECE 273 Convex Optimization and Applications]

~~~

~~~
* Optimization and Machine Learning Group: *

PhD students:

- [https://canchen-cc.github.io/ Can Chen] \[HDSI\], since 2024 - 
- Maria-Eleni Sfyraki \[HDSI\], since 2024 - 

MS students:

- Yi Liu \[ECE\], since 2024 -

~~~

~~~
*Preprints: * 

[https://arxiv.org/abs/2410.22318 Online Detecting LLM-Generated Texts via Sequential Hypothesis Testing by Betting.] \n Can Chen and Jun-Kun Wang. \n
arXiv:2410.22318. 2024.

[https://arxiv.org/abs/2402.13988 Hamiltonian Descent and Coordinate Hamiltonian Descent.] \n Jun-Kun Wang. \n
 arXiv:2402.13988. 2024.

*Publications:  \*Corresponding Author/\*Presenting Author *

[https://link.springer.com/article/10.1007/s10107-023-01976-y No-Regret Dynamics in the Fenchel Game: A Unified Framework for Algorithmic Convex Optimization.] \n
Jun-Kun Wang, Jacob Abernethy, Kfir Y. Levy. \n
[https://link.springer.com/article/10.1007/s10107-023-01976-y Mathematical Programming 2023.] 

[https://arxiv.org/abs/2207.02189 Accelerating Hamiltonian Monte Carlo via Chebyshev Integration Time] \n
Jun-Kun Wang and Andre Wibisono \n
In ICLR /(International Conference on Learning Representations)/, 2023.

[https://openreview.net/forum?id=yYbhKqdi7Hz Continuized Acceleration for Quasar Convex Functions in Non-Convex Optimization] \n
Jun-Kun Wang and Andre Wibisono \n
In ICLR /(International Conference on Learning Representations)/, 2023.

[https://arxiv.org/abs/2210.10019 Towards Understanding GD with Hard and Conjugate Pseudo-labels for Test-Time Adaptation] \n
Jun-Kun Wang and Andre Wibisono \n
In ICLR /(International Conference on Learning Representations)/, 2023.

[https://arxiv.org/abs/2206.11872   Provable Acceleration of Heavy Ball beyond Quadratics for a class of Polyak-Lojasiewicz Functions when the Non-Convexity is Averaged-Out] \n
Jun-Kun Wang, Chi-Heng Lin, Andre Wibisono, and Bin Hu \n
In ICML /(International Conference on Machine Learning)/, 2022.

[https://arxiv.org/abs/2106.12923 Understanding Modern Techniques in Optimization: Frank-Wolfe, Nesterov's Momentum, and Polyak's Momentum.] \n
PhD Dissertation at Georgia Tech. 2021.

[https://arxiv.org/abs/2010.01618 A Modular Analysis of Provable Acceleration via Polyak's momentum: Training a Wide ReLU Network and a Deep Linear Network ] \n
Jun-Kun Wang, Chi-Heng Lin, and Jacob Abernethy. \n 
In ICML /(International Conference on Machine Learning)/, 2021.

[https://arxiv.org/abs/2010.01637 Understanding How Over-Parametrization Leads to Acceleration: A case of learning a single teacher neuron] \n
Jun-Kun Wang and Jacob Abernethy. \n
In ACML /(Asian Conference on Machine Learning)/, 2021.

[https://openreview.net/forum?id=rkeNfp4tPr  Escape Saddle Points Faster with Stochastic Momentum]. \n
Jun-Kun Wang, Chi-Heng Lin, and Jacob Abernethy. \n
In ICLR /(International Conference on Learning Representations)/, 2020.

[http://proceedings.mlr.press/v98/wang19b/wang19b.pdf Online Linear Optimization with Sparsity Constraints] \n
*\**Jun-Kun Wang, *\** Chi-Jen Lu, and Shou-De Lin. \n
In ALT /(International Conference on Algorithmic Learning Theory)/, 2019.

[https://arxiv.org/abs/1811.05831  Revisiting Projection-Free Optimization For Strongly Convex Constraint Sets] \n
Jarrid Rector-Brooks, Jun-Kun Wang, and Barzan Mozafari. \n
In /AAAI 33, 2019/.

[https://arxiv.org/abs/1807.10455 Acceleration through Optimistic No-Regret Dynamics] \n
*\**Jun-Kun Wang and Jacob Abernethy. \n 
In NeurIPS (/Annual Conference on Neural Information Processing Systems/), 2018. 
(Spotlight) [https://arxiv.org/pdf/1807.10455.pdf paper]

[http://proceedings.mlr.press/v75/abernethy18a Faster Rates for Convex-Concave Games] \n
(name order) Jacob Abernethy, Kevin Lai, Kfir Levy, and *\**Jun-Kun Wang. \n
In COLT /(Computational Learning Theory)/, 2018.

[https://papers.nips.cc/paper/2017/hash/7371364b3d72ac9a3ed8638e6f0be2c9-Abstract.html On Frank-Wolfe and Equilibrium Computation]\n
Jacob Abernethy and *\**Jun-Kun Wang. \n
In NeurIPS (/Annual Conference on Neural Information Processing Systems/), 2017. 
(Spotlight)
[./papers/nips_2017.pdf paper] [./papers/nips17_supp.pdf supplementary]

[./papers/dsaa2016_admm.pdf Efficient Sampling-based ADMM for Distributed Data] \n *\**Jun-Kun Wang, Shou-De Lin.\n
In DSAA /(IEEE International Conference on Data Science and Advanced Analytics)/, 2016. [https://github.com/j123456/dis_sdcaadmm code]

[./papers/dsaa2016_plsi.pdf Parallel Least-Squares Policy Iteration] \n
*\**Jun-Kun Wang, Shou-De Lin. \n
In DSAA /(IEEE International Conference on Data Science and Advanced Analytics)/,
2016.

[https://proceedings.mlr.press/v32/wangf14.html Robust Inverse Covariance Estimation under Noisy Measurements] \n *\**Jun-Kun Wang, Shou-De Lin.\n
In ICML /(International Conference on Machine Learning)/, 2014.
[http://jmlr.org/proceedings/papers/v32/wangf14.pdf paper]
[./papers/slide_icml14.pdf slide]
~~~


