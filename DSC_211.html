<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="stylesheet" href="jemdocCustom.css" type="text/css" />
<title>DSC211 - UC San Diego</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Jun-Kun Wang</div>
<div class="menu-item"><a href="index.html">home</a></div>
</td>
<td id="layout-content">
<h1>DSC 211, Winter 2024 &ndash; Introduction to Optimization</h1>
<p><b>Lectures</b>: Tuesdays/Thursdays 5:00 pm -6:20 pm PST 
</p>
<p><b>Location</b>: PCYNH (Pepper Canyon Hall)  121
</p>
<p><b>Instructors</b>: 
</p>
<ul>
<li><p><a href="https://jimwang123.github.io" target=&ldquo;blank&rdquo;>Jun-Kun Wang</a> (jkw005 at ucsd.edu)
</p>
</li>
<li><p>Yuan (Merlin) Chang 
</p>
</li>
<li><p>Marialena Sfyraki
</p>
</li>
</ul>
<p><b>Office hours</b>:
</p>
<ul>
<li><p>Jun-Kun Wang  (3PM-4PM, Jacobs Hall Room 6404)
</p>
</li>
<li><p>Yuan (Merlin) Chang  (Monday 1PM-2PM, HDSI Room 336)
</p>
</li>
<li><p>Marialena Sfyraki  (Wednesday 5PM-6PM, HDSI Room 355) 
</p>
</li>
</ul>
<p><b>Syllabus</b>: <a href="https://canvas.ucsd.edu/courses/51990" target=&ldquo;blank&rdquo;>Available on Canvas</a>
</p>
<h2>Course description</h2>
<p>This course covers the theoretical and algorithmic foundations of optimization.
We will walk through classical optimization algorithms such as Gradient Descent, Coordinate
Descent, the Frank-Wolfe Method, Accelerated Methods, Mirror Descent, Stochastic Gradient
Descent, and Online Gradient Descent. We will put particular emphasis on understanding the
behaviors and the convergence rate guarantees of these algorithms, as well as the tools and techniques
to analyze them. We will also cover some convex analysis and duality theory. Students will
learn the basic foundations of deterministic convex optimization, stochastic optimization, online
convex optimization, min-max optimization, and non-convex optimization
</p>
<p>A tentative list of topics that we will cover include
</p>
<ul>
<li><p>Mathematical background and Gradient Flow.
</p>
</li>
<li><p>Convexity. 
</p>
</li>
<li><p>Gradient Descent for smooth, smooth and strongly convex functions.
</p>
</li>
<li><p>Reduction
</p>
</li>
<li><p>Projected Gradient Descent and Frank-Wolfe Method.
</p>
</li>
<li><p>Coordinate Descent
</p>
</li>
<li><p>Duality Theory; Stochastic Dual Coordinate Ascent
</p>
</li>
<li><p>Mirror Descent
</p>
</li>
<li><p>SGD
</p>
</li>
<li><p>SGD and variance reduction
</p>
</li>
<li><p>Online Convex Optimization
</p>
</li>
<li><p>Min-Max Optimization
</p>
</li>
<li><p>Acceleration via the Chebyshev Polynomial
</p>
</li>
<li><p>Momentum Methods
</p>
</li>
<li><p>Nonconvex optimization 
</p>
</li>
</ul>
<p>Students are expected to sign up on Piazza and GradeScope <a href="Code:" target=&ldquo;blank&rdquo;>B2XR7X</a>. Discussions will happen on Piazza. The homework should be turned in and will be graded on GradeScope. Important announcements will be made via Canvas. 
</p>
<h2>Pre-requisites</h2>
<p>This course assumes basic knowledge in linear algebra and calculus. 
</p>
<h2>Course grade</h2>
<ul>
<li><p>40% homework (5 problem sets; will drop the lowest score)
</p>
</li>
<li><p>25% midterm exam (in class; one page of hand-written cheat sheet allowed)
</p>
</li>
<li><p>25% final project
</p>
</li>
<li><p>10% scribing and 5% extra credit for course attendance/participation
</p>
</li>
</ul>
<h2>Schedule</h2>
<table id="schedule">
<tr class="r1"><td class="c1">Week </td><td class="c2"> Date   </td><td class="c3">   Lecture                                </td><td class="c4"> Scribe note </td><td class="c5"> Assignments </td></tr>
<tr class="r2"><td class="c1">1    </td><td class="c2"> Jan 09 </td><td class="c3">   L1: Introduction &amp; Course Logistics    </td><td class="c4">         </td><td class="c5"> HW1 out   </td></tr>
<tr class="r3"><td class="c1"></td><td class="c2"> Jan 11 </td><td class="c3">   L2: Mathematical background and Gradient Flow     </td><td class="c4">  <a href="./DSC211_Scribe/scribe2.pdf" target=&ldquo;blank&rdquo;>scribe2</a>     </td><td class="c5">   </td></tr>
<tr class="r4"><td class="c1">2    </td><td class="c2"> Jan 16 </td><td class="c3">   L3: Convexity and Gradient Descent            </td><td class="c4">    </td><td class="c5">         </td></tr>
<tr class="r5"><td class="c1"></td><td class="c2"> Jan 18 </td><td class="c3">   L4: Gradient Descent and Reduction         </td><td class="c4">        </td><td class="c5">     </td></tr>
<tr class="r6"><td class="c1">3    </td><td class="c2"> Jan 23 </td><td class="c3">   L5: Projected Gradient Descent and Frank-Wolfe Method </td><td class="c4">  </td><td class="c5"> HW1 due; HW2 out </td></tr>
<tr class="r7"><td class="c1"></td><td class="c2"> Jan 25 </td><td class="c3">   L6: Coordinate Descent         </td><td class="c4">          </td><td class="c5">   </td></tr>
<tr class="r8"><td class="c1">4    </td><td class="c2"> Jan 30 </td><td class="c3">   L7: Intoduction to Stochastic Optimization   </td><td class="c4">  </td><td class="c5">           </td></tr>
<tr class="r9"><td class="c1"></td><td class="c2"> Feb 01 </td><td class="c3">   L8: SGD variants  </td><td class="c4">  </td><td class="c5"> HW2 due; HW3 out </td></tr>
<tr class="r10"><td class="c1">5    </td><td class="c2"> Feb 06 </td><td class="c3">   L9: Duality Theory    </td><td class="c4">  </td><td class="c5">           </td></tr>
<tr class="r11"><td class="c1"></td><td class="c2"> Feb 08 </td><td class="c3">   L10: Duality Theory and Stochastic Dual Coordinate Ascent  </td><td class="c4">  </td><td class="c5">  </td></tr>
<tr class="r12"><td class="c1">6    </td><td class="c2"> Feb 13 </td><td class="c3">   L11: Mirror Descent    </td><td class="c4">  </td><td class="c5">  HW3 due; HW4 out </td></tr>
<tr class="r13"><td class="c1"></td><td class="c2"> Feb 15 </td><td class="c3">   L12: Online Convex Optimization                   </td><td class="c4">     </td><td class="c5">       </td></tr>
<tr class="r14"><td class="c1">7    </td><td class="c2"> Feb 20 </td><td class="c3">   L13: Online Convex Optimization              </td><td class="c4">      </td><td class="c5">   </td></tr>
<tr class="r15"><td class="c1"></td><td class="c2"> Feb 22 </td><td class="c3">   L14: Min-Max Optimization          </td><td class="c4">  </td><td class="c5"> HW4 due; <b> Checkpoint Report Due </b>      </td></tr>
<tr class="r16"><td class="c1">8    </td><td class="c2"> Feb 27 </td><td class="c3">   L15  Min-Max Optimization </td><td class="c4">        </td><td class="c5">         </td></tr>
<tr class="r17"><td class="c1"></td><td class="c2"> Feb 29 </td><td class="c3">   <b> Midterm </b>    </td><td class="c4">       </td><td class="c5">     </td></tr>
<tr class="r18"><td class="c1">9    </td><td class="c2"> Mar 05 </td><td class="c3">   L16: Acceleration via Chebyshev Polynomial </td><td class="c4">  </td><td class="c5">  HW5 out         </td></tr>
<tr class="r19"><td class="c1"></td><td class="c2"> Mar 07 </td><td class="c3">   L17: Momentum Methods  </td><td class="c4">        </td><td class="c5">      </td></tr>
<tr class="r20"><td class="c1">10   </td><td class="c2"> Mar 12 </td><td class="c3">   L18: Introduction to Non-Convex Optimization   </td><td class="c4">      </td><td class="c5">      </td></tr>
<tr class="r21"><td class="c1"></td><td class="c2"> Mar 14 </td><td class="c3">   L19: Concluding remarks   </td><td class="c4">   </td><td class="c5">        </td></tr>
<tr class="r22"><td class="c1">11   </td><td class="c2"> Mar 21 </td><td class="c3"> Project Presentation        </td><td class="c4">           </td><td class="c5"> <b> Written Report Due </b>    </td></tr>
<tr class="r23"><td class="c1">
</td></tr></table>
<h2>Final Projects</h2>
<p>Final project: You can form a group of size up to 3 members for the final project. 
</p>
<p><b> Grading of the final project: </b>  1% for a checkpoint report, 12% for a written report, 12 % for an oral presentation.  
</p>
<p><b> Written report of the final project : </b>
</p>
<ul>
<li><p>Describe an optimization paper in detail using your own words. The paper you focus on should be a peer-reviewed paper from top venues such as ICML, NeurIPS, ICLR, COLT, JMLR, Mathematical Programming, or SIAM Journal on Optimization from the past three years. Focusing on papers not from these venues requires discussion with the instructor and obtaining approval first.
</p>
</li>
</ul>
<p>Students are required to <b>submit a typed write-up</b> for the final project consisting of 4-6 pages (the list of references does not count toward the page limit). Submitting a report that is less than 4 full pages (references excluded) will result in a score deduction.  The write-up must contain a significant theoretical component. Please use the Latex template provided in this link <a href="https://neurips.cc/Conferences/2023/PaperInformation/StyleFiles" target=&ldquo;blank&rdquo;>Latex Template</a>
</p>
<p>The report should include <b> the motivation, a description of the algorithm, the underlying algorithmic design principles, and a clear reproduction or summary of the theoretical analysis </b>. 
When reproducing the proof, ask yourself: What is the most essential part of the analysis? Can you significantly simplify the proofs without trivializing the results? Because of the page limit, you should avoid presenting every step in the analysis in your written report. Focus on conveying the logical flow and key steps of the theoretical analysis.
</p>
<p>Students are encouraged to perform simple simulations to test the proposed algorithm(s) in the paper
and report their findings in the report, if the paper includes experiments. Code snippets and additional simulation results can be provided in the appendix (after the list of references if any) and do not contribute to the page limit, but the instructor does not have an obligation to read the appendix. The simulations of the method(s) in the paper can be conducted using small, toy, and synthetic data created by your own to showcase the convergent behavior of the algorithm(s) and their baselines. Students do not need to precisely follow the setting nor use the same data as described in the paper. Some papers are purely theoretical without any numerical experiments. In this case, students are still encouraged to create their own synthetic data or come up with a simulation scenario to verify the theoretical results. However, the evaluation of the written report will be focused on whether you can present the motivation, the algorithm, the idea behind the algorithm design, and reproducing/summarizing the analysis clearly. 
Simulations are optional and will only be considered for the assessment if the other aforementioned components of the written report are not strong.  
</p>
<p><b> The due date of the written report is Thursday, March 21, 2024, 6:00 pm.  </b>
</p>
<p><b> Additional Tasks (Optional) </b> include (1)
identifying the limitations and proposing a way to overcome them (theoretically and/or empirically), (2) or improving the results in the paper, such as obtaining a tighter bound or modifying the algorithms to be faster (theoretically and/or empirically), (3) or proposing a new algorithm in the same setting. Final projects which include an additional task with a  <b> solid </b> result may receive extra credits.
If an additional task is conducted, students should highlight what they have accomplished in the main text of the written report and may provide further detail in the appendix after Page 6, but the instructor does not have an obligation to read the appendix. 
</p>
<p><b> Final presentation for the project: </b>
</p>
<p>The session for oral presentations is scheduled for <b> Thursday, March 21, 2024, from 7:00 pm to 9:59 pm </b>  (the final exam time).  
Each team has 8 minutes (subject to change) for their presentation and 1 minute (subject to change) for the Q&amp;A.  A team can earn extra credits in their oral presentation by asking a good question or making a constructive remark on the presentations of other teams during the Q&amp;A.
</p>
<p>Given the limited time available for each oral presentation, each team should not walk through the proof but a discussion of theoretical results (e.g. convergence rates) and/or empirical results is encouraged. 
The first couple of minutes/slides should be allocated to describe the setup as clear as possible and/or provide necessary background so that the audience can be on the same page. Subsequently, the main results of the paper should be clearly summarized. If you think you have completed an additional task listed in Additional Tasks above, make sure to allocate at least one-third of time to highlight what you have accomplished. 
</p>
<p><b> Checkpoint report of the Final Project: </b>
In the checkpoint report, students are required to include the following information:
</p>
<ul>
<li><p>The name of each team member.
</p>
</li>
<li><p>The title of the paper that you will focus on.
</p>
</li>
</ul>
<p>It is emphasized that every member in the team should submit a checkpoint report. The checkpoint report is due on <b> Feb. 23 Friday at 5PM </b>.
</p>
<h2>References</h2>
<p>There is no required textbooks. The course will draw some of the materials from the following references. All the following materials are accessible online or accessible via UCSD library online portal. 
</p>
<ul>
<li><p>Sebastien Bubeck <a href="https://arxiv.org/pdf/1405.4980.pdf" target=&ldquo;blank&rdquo;>Convex Optimization: Algorithms and Complexity</a>. 
</p>
</li>
</ul>
<ul>
<li><p>Nisheeth K. Vishnoi <a href="https://convex-optimization.github.io/" target=&ldquo;blank&rdquo;>Algorithms for Convex Optimization</a>
</p>
</li>
</ul>
<ul>
<li><p>Francesco Orabona <a href="https://arxiv.org/pdf/1912.13213.pdf" target=&ldquo;blank&rdquo;>A Modern Introduction to Online Learning</a>
</p>
</li>
</ul>
<ul>
<li><p>Aaron Sidford <a href="https://drive.google.com/file/d/1BfMkt2glaZpJGwg7gwsJw9T_XxH3o8gx/view" target=&ldquo;blank&rdquo;>Optimization Algorithms</a>
</p>
</li>
</ul>
<ul>
<li><p>Stephen Boyd and Lieven Vandenberghe <a href="https://web.stanford.edu/~boyd/cvxbook/" target=&ldquo;blank&rdquo;>Convex Optimization</a>
</p>
</li>
</ul>
<ul>
<li><p>Shai Shalev-Shwartz <a href="https://www.cs.huji.ac.il/~shais/papers/OLsurvey.pdf" target=&ldquo;blank&rdquo;>Online Learning and Online Convex Optimization</a>
</p>
</li>
</ul>
<ul>
<li><p>Yin Tat Lee and Santosh Vempala <a href="https://github.com/YinTat/optimizationbook/blob/main/main.pdf" target=&ldquo;blank&rdquo;>Techniques in Optimization and Sampling</a>
</p>
</li>
</ul>
<ul>
<li><p>Arkadi Nemirovski <a href="https://www2.isye.gatech.edu/~nemirovs/LMCOLN2022Fall.pdf" target=&ldquo;blank&rdquo;>Lectures on Modern Convex Optimization</a>
</p>
</li>
</ul>
<ul>
<li><p>Elad Hazan <a href="https://arxiv.org/pdf/1909.05207.pdf" target=&ldquo;blank&rdquo;>Introduction to Online Convex Optimization</a>
</p>
</li>
</ul>
<ul>
<li><p>John Duchi <a href="https://stanford.edu/~jduchi/PCMIConvex/Duchi16.pdf" target=&ldquo;blank&rdquo;>Introductory Lectures on Stochastic Convex Optimization</a>
</p>
</li>
</ul>
<ul>
<li><p>Alexandre d'Aspremont, Damien Scieur, Adrien Taylor <a href="https://arxiv.org/abs/2101.09545.pdf" target=&ldquo;blank&rdquo;>Acceleration Methods</a>
</p>
</li>
</ul>
<ul>
<li><p>Benjamin Recht and Stephen J. Wright <a href="https://people.eecs.berkeley.edu/~brecht/opt4ml_book/" target=&ldquo;blank&rdquo;>Optimization for Modern Data Analysis</a>
</p>
</li>
</ul>
<ul>
<li><p>Guanghui Lan <a href="https://link.springer.com/book/10.1007/978-3-030-39568-1" target=&ldquo;blank&rdquo;>First-order and Stochastic Optimization Methods for Machine Learning</a>
</p>
</li>
</ul>
<ul>
<li><p>Yurii Nesterov <a href="https://link.springer.com/book/10.1007/978-1-4419-8853-9" target=&ldquo;blank&rdquo;>Introductory Lectures on Convex Optimization</a>
</p>
</li>
</ul>
<ul>
<li><p>Amir Beck <a href="https://epubs.siam.org/doi/book/10.1137/1.9781611977622" target=&ldquo;blank&rdquo;>Introduction to Nonlinear Optimization - Theory, Algorithms and Applications</a>
</p>
</li>
</ul>
<h2>Acknowledgments</h2>
<p>The design of this course is inspired by the following excellent courses:
</p>
<ul>
<li><p><a href="https://ee227c.github.io/" target=&ldquo;blank&rdquo;>Convex Optimization and Approximation</a> at UC Berkeley by Moritz Hardt. 
</p>
</li>
</ul>
<ul>
<li><p><a href="https://yuxinchen2020.github.io/large_scale_optimization/index.html" target=&ldquo;blank&rdquo;>Large-Scale Optimization for Data Science</a> at University of Pennsylvania by Yuxin Chen. 
</p>
</li>
</ul>
<ul>
<li><p><a href="https://optmlclass.github.io/" target=&ldquo;blank&rdquo;>Optimization for Machine Learning</a> at Boston University by Ashok Cutkosky.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://web.stanford.edu/~sidford/courses/19fa_opt_theory/fa19_opt_theory.html" target=&ldquo;blank&rdquo;>Introduction to Optimization Theory</a> at Standford by Aaron Sidford
</p>
</li>
</ul>
<p>The course webpage and the course policy is inspired by the following excellent courses:
</p>
<ul>
<li><p><a href="https://zhengy09.github.io/ECE285/ece285.html" target=&ldquo;blank&rdquo;>Semidefinite and Sum-of-squares Optimization</a> at UC San Diego by Yang Zheng
</p>
</li>
<li><p><a href="https://mltheory.github.io/CS7545/" target=&ldquo;blank&rdquo;>Machine Learning Theory</a> at Georgia Tech by Jacob Abernethy
</p>
</li>
<li><p><a href="https://nanjiang.cs.illinois.edu/cs542/" target=&ldquo;blank&rdquo;>Statistical Reinforcement Learning</a> at UIUC by Nan Jiang
</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2024-01-24 16:08:27 PST, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
