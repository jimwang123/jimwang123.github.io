<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>ECE273 - UC San Diego</title>
</head>
<body>
<div id="layout-content">
<h1>ECE 273, Spring 2024 &ndash; Convex Optimization and Applications</h1>
<p><b>Lectures</b>: TBD
</p>
<p><b>Location</b>: TBD
</p>
<p><b>Instructors</b>: 
</p>
<ul>
<li><p><a href="https://jimwang123.github.io" target=&ldquo;blank&rdquo;>Jun-Kun Wang</a> (jkw005 at ucsd.edu)
</p>
</li>
</ul>
<p><b>Office hours</b>:
</p>
<ul>
<li><p>TBD
</p>
</li>
</ul>
<p><b>Syllabus</b>: Available on Canvas
</p>
<h2>Course description</h2>
<p>This course covers the theoretical and algorithmic foundations of optimization.
We will walk through classical optimization algorithms such as Gradient Descent, Coordinate
Descent, the Frank-Wolfe Method, Accelerated Methods, Mirror Descent, Stochastic Gradient
Descent, and Online Gradient Descent. We will put particular emphasis on understanding the
behaviors and the convergence rate guarantees of these algorithms, as well as the tools and techniques
to analyze them. We will also cover some convex analysis and duality theory. Students will
learn the basic foundations of deterministic convex optimization, stochastic optimization, online
convex optimization, min-max optimization, and non-convex optimization
</p>
<p>A tentative list of topics that we will cover include
</p>
<ul>
<li><p>Mathematical background and Gradient Flow.
</p>
</li>
<li><p>Convexity. 
</p>
</li>
<li><p>Gradient Descent for smooth, smooth and strongly convex functions.
</p>
</li>
<li><p>Reduction
</p>
</li>
<li><p>Projected Gradient Descent and Frank-Wolfe Method.
</p>
</li>
<li><p>Coordinate Descent
</p>
</li>
<li><p>Duality Theory; Stochastic Dual Coordinate Ascent
</p>
</li>
<li><p>Mirror Descent
</p>
</li>
<li><p>SGD
</p>
</li>
<li><p>SGD and variance reduction
</p>
</li>
<li><p>Online Convex Optimization
</p>
</li>
<li><p>Min-Max Optimization
</p>
</li>
<li><p>Nonconvex optimization 
</p>
</li>
<li><p>Acceleration via the Chebyshev Polynomial
</p>
</li>
<li><p>Momentum Methods
</p>
</li>
<li><p>Newton Method
</p>
</li>
</ul>
<p>Students are expected to sign up on Piazza and GradeScope. Discussions will happen on Piazza. The homework should be turned in and will be graded on GradeScope. Important announcements will be made via Canvas. 
</p>
<h2>Pre-requisites</h2>
<p>This course assumes basic knowledge in linear algebra and calculus. 
</p>
<h2>Course grade</h2>
<ul>
<li><p>40% homework (5 problem sets; will drop the lowest score)
</p>
</li>
<li><p>25% midterm exam (in class; one page of hand-written cheat sheet allowed)
</p>
</li>
<li><p>25% final exam
</p>
</li>
<li><p>10% scribing and 5% extra credit for course attendance/participation
</p>
</li>
</ul>
<h2>Homework Policy</h2>
<p>Students can work with up to two other classmates on the homeworks. 
However, the solutions submitted must be independently written in your own words.
As a hard rule, you must write up your solution individually and cannot copy your classmates&rsquo; solutions or let your classmates copy your solutions; otherwise, you may not receive any credit for the HW problem(s). You must write your collaborators’ names on the top of your assignment. If you do not work with collaborators, list “Collaborators: None.”
</p>
<p>Students are encouraged to submit a typed solution. Written solutions are allowed but have to be clearly readable. Students might not receive credit for the problems if the written solutions are incomprehensible. We reserve the right to determine if a written solution is incomprehensible.
</p>
<p>Each homework and project assignment will have a clearly stated due date. We expect you to turn in all completed problem sets on time. Late submissions and deadline extensions will not be possible for any reason.
</p>
<h2>Scribe Policy</h2>
<p>Starting from the second lecture, each lecture will have two to three students to scribe. This group of students will be responsible for taking detailed notes in class, and the quality of these notes will be graded. There will be a sign-up sheet for students to sign up to scribe for a preferred date.
</p>
<p>Students should use the provided scribe template available on Canvas and email a typed PDF file with the LaTeX code within 7 days to the TA. The TA will review the scribe and may request students to make updates if any part of the notes is unclear or contains errors. It is expected that students revise the scribe accordingly within 2 days. Students will receive no more than one request if there are issues in the scribe. If students do not address the issues or errors in the scribe timely, 
then TAs may update the scribe; in this case, students may not receive full credit. 
</p>
<h2>Academic Integrity</h2>
<p><a href="https://academicintegrity.ucsd.edu/" target=&ldquo;blank&rdquo;>UCSD's Code of Academic Integrity</a> applies to this course. It is dishonest to cheat on exams, copy other people's work, or fake experimental results. An important element of academic integrity is fully and correctly acknowledging any materials taken from the work of others. Instances of academic dishonesty will be referred to the Office of Student Conduct for adjudication.
</p>
<h2>References</h2>
<p>There is no required textbooks. The course will draw some of the materials from the following references. All the following materials are accessible online or accessible via UCSD library online portal. 
</p>
<ul>
<li><p>Sebastien Bubeck <a href="https://arxiv.org/pdf/1405.4980.pdf" target=&ldquo;blank&rdquo;>Semidefinite optimization andconvex algebraic geometry</a>. 
</p>
</li>
</ul>
<ul>
<li><p>Nisheeth K. Vishnoi <a href="https://convex-optimization.github.io/" target=&ldquo;blank&rdquo;>Algorithms for Convex Optimization</a>
</p>
</li>
</ul>
<ul>
<li><p>Francesco Orabona <a href="https://arxiv.org/pdf/1912.13213.pdf" target=&ldquo;blank&rdquo;>A Modern Introduction to Online Learning</a>
</p>
</li>
</ul>
<ul>
<li><p>Aaron Sidford <a href="https://drive.google.com/file/d/1BfMkt2glaZpJGwg7gwsJw9T_XxH3o8gx/view" target=&ldquo;blank&rdquo;>Optimization Algorithms</a>
</p>
</li>
</ul>
<ul>
<li><p>Stephen Boyd and Lieven Vandenberghe <a href="https://web.stanford.edu/~boyd/cvxbook/" target=&ldquo;blank&rdquo;>Convex Optimization</a>
</p>
</li>
</ul>
<ul>
<li><p>Yin Tat Lee and Santosh Vempala <a href="https://github.com/YinTat/optimizationbook/blob/main/main.pdf" target=&ldquo;blank&rdquo;>Techniques in Optimization and Sampling</a>
</p>
</li>
</ul>
<ul>
<li><p>Arkadi Nemirovski <a href="https://www2.isye.gatech.edu/~nemirovs/LMCOLN2022Fall.pdf" target=&ldquo;blank&rdquo;>Lectures on Modern Convex Optimization</a>
</p>
</li>
</ul>
<ul>
<li><p>Elad Hazan <a href="https://arxiv.org/pdf/1909.05207.pdf" target=&ldquo;blank&rdquo;>Introduction to Online Convex Optimization</a>
</p>
</li>
</ul>
<ul>
<li><p>Alexandre d'Aspremont, Damien Scieur, Adrien Taylor <a href="https://arxiv.org/abs/2101.09545.pdf" target=&ldquo;blank&rdquo;>Acceleration Methods</a>
</p>
</li>
</ul>
<ul>
<li><p>Benjamin Recht and Stephen J. Wright <a href="https://people.eecs.berkeley.edu/~brecht/opt4ml_book/" target=&ldquo;blank&rdquo;>Optimization for Modern Data Analysis</a>
</p>
</li>
</ul>
<ul>
<li><p>Guanghui Lan <a href="https://link.springer.com/book/10.1007/978-3-030-39568-1" target=&ldquo;blank&rdquo;>First-order and Stochastic Optimization Methods for Machine Learning</a>
</p>
</li>
</ul>
<ul>
<li><p>Yurii Nesterov <a href="https://link.springer.com/book/10.1007/978-1-4419-8853-9" target=&ldquo;blank&rdquo;>Introductory Lectures on Convex Optimization</a>
</p>
</li>
</ul>
<ul>
<li><p>Amir Beck <a href="https://epubs.siam.org/doi/book/10.1137/1.9781611977622" target=&ldquo;blank&rdquo;>Introduction to Nonlinear Optimization - Theory, Algorithms and Applications</a>
</p>
</li>
</ul>
<h2>Acknowledgments</h2>
<p>The design of this course is inspired by the following excellent courses:
</p>
<ul>
<li><p><a href="https://ee227c.github.io/" target=&ldquo;blank&rdquo;>Convex Optimization and Approximation</a> at UC Berkeley by Moritz Hardt. 
</p>
</li>
</ul>
<ul>
<li><p><a href="https://yuxinchen2020.github.io/large_scale_optimization/index.html" target=&ldquo;blank&rdquo;>Large-Scale Optimization for Data Science</a> at University of Pennsylvania by Yuxin Chen. 
</p>
</li>
</ul>
<ul>
<li><p><a href="https://optmlclass.github.io/" target=&ldquo;blank&rdquo;>Optimization for Machine Learning</a> at Boston University by Ashok Cutkosky.
</p>
</li>
</ul>
<p>The course webpage and the course policy is inspired by the following excellent courses:
</p>
<ul>
<li><p><a href="https://zhengy09.github.io/ECE285/ece285.html" target=&ldquo;blank&rdquo;>Semidefinite and Sum-of-squares Optimization</a> at UC San Diego by Yang Zheng
</p>
</li>
<li><p><a href="https://mltheory.github.io/CS7545/" target=&ldquo;blank&rdquo;>Machine Learning Theory</a> at Georgia Tech by Jacob Abernethy
</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2024-01-19 12:33:50 PST, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</div>
</body>
</html>
