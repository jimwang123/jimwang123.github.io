<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="stylesheet" href="jemdocCustom.css" type="text/css" />
<title>ECE273 - UC San Diego</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Jun-Kun Wang</div>
<div class="menu-item"><a href="index.html">home</a></div>
</td>
<td id="layout-content">
<h1>ECE 273, Spring 2024 &ndash; Convex Optimization and Applications</h1>
<p><b>Lectures</b>: Tuesdays/Thursdays 3:30 pm -4:50 pm PST 
</p>
<p><b>Location</b>: PETER (Peterson Hall) 102
</p>
<p><b>Instructors</b>: 
</p>
<ul>
<li><p><a href="https://jimwang123.github.io" target=&ldquo;blank&rdquo;>Jun-Kun Wang</a> (jkw005 at ucsd.edu)
</p>
</li>
<li><p>TA: Marialena Sfyraki
</p>
</li>
</ul>
<p><b>Office hours</b>:
</p>
<ul>
<li><p>Jun-Kun Wang  (Friday 3PM-4PM, Jacobs Hall Room 6404)
</p>
</li>
<li><p>Marialena Sfyraki  (Wednesday 1PM-2PM, Jacobs Hall Room 4506) 
</p>
</li>
</ul>
<p><b>Syllabus</b>: <a href="https://canvas.ucsd.edu/courses/55130" target=&ldquo;blank&rdquo;>Available on Canvas</a>
</p>
<h2>Course description</h2>
<p>This course covers the theoretical and algorithmic foundations of optimization. We will cover some convex analysis and duality theory, and we will walk through classical optimization algorithms such as Gradient Descent, Coordinate Descent, the Frank-Wolfe Method, Accelerated Methods, Mirror Descent, Stochastic Gradient Descent, and Online Gradient Descent. We will put particular emphasis on understanding the behaviors and the convergence rate guarantees of these algorithms, as well as the tools and techniques to analyze them. Students will learn the basic foundations of deterministic convex optimization, stochastic optimization, online convex optimization, min-max optimization, and non-convex optimization.
</p>
<p>A tentative list of topics that we will cover include
</p>
<ul>
<li><p>Mathematical background and Gradient Flow.
</p>
</li>
<li><p>Convexity, strong convexity, smoothness of a function.
</p>
</li>
<li><p>Gradient Descent and its convergence analysis
</p>
</li>
<li><p>Reduction
</p>
</li>
<li><p>Properties of convex sets
</p>
</li>
<li><p>Subgradients and optimality conditions
</p>
</li>
<li><p>Projected Gradient Descent and Frank-Wolfe Method
</p>
</li>
<li><p>Coordinate Descent
</p>
</li>
<li><p>Duality Theory, KKT conditions, and Stochastic Dual Coordinate Ascent
</p>
</li>
<li><p>Mirror Descent
</p>
</li>
<li><p>SGD
</p>
</li>
<li><p>SGD and variance reduction
</p>
</li>
<li><p>Online Convex Optimization
</p>
</li>
<li><p>Min-Max Optimization
</p>
</li>
<li><p>Nonconvex optimization (Gradient Dominance Condition, Finding a stationary point)
</p>
</li>
<li><p>Acceleration via the Chebyshev Polynomial
</p>
</li>
<li><p>Momentum methods
</p>
</li>
<li><p>Optimization as a game
</p>
</li>
</ul>
<p>Students are expected to sign up on Piazza and GradeScope. Discussions will happen on Piazza. The homework should be turned in and will be graded on GradeScope. Important announcements will be made via Canvas. 
</p>
<h2>Pre-requisites</h2>
<p>This course assumes basic knowledge in linear algebra and calculus. 
</p>
<h2>Course grade</h2>
<ul>
<li><p>40% homework (5 problem sets; will drop the lowest score)
</p>
</li>
<li><p>25% midterm exam (in class; one page of hand-written cheat sheet allowed)
</p>
</li>
<li><p>25% final project
</p>
</li>
<li><p>10% scribing and 5% extra credit for course attendance/participation
</p>
</li>
</ul>
<h2>Schedule</h2>
<table id="schedule">
<tr class="r1"><td class="c1">Week </td><td class="c2"> Date   </td><td class="c3">   Lecture                                </td><td class="c4"> Scribe note </td><td class="c5"> Assignments </td></tr>
<tr class="r2"><td class="c1">1    </td><td class="c2"> Apr 02 </td><td class="c3">   L1: Course logistics, Mathematical background   </td><td class="c4">   <a href="./ECE273_Scribe/scribe_L1.pdf" target=&ldquo;blank&rdquo;>scribe1</a>      </td><td class="c5"> HW1 out   </td></tr>
<tr class="r3"><td class="c1"></td><td class="c2"> Apr 04 </td><td class="c3">   L2: Gradient Flow and Convex Analysis I     </td><td class="c4">  <a href="./ECE273_Scribe/scribe_L2.pdf" target=&ldquo;blank&rdquo;>scribe2</a>     </td><td class="c5">   </td></tr>
<tr class="r4"><td class="c1">2    </td><td class="c2"> Apr 09 </td><td class="c3">   L3: Gradient Descent            </td><td class="c4"> <a href="./ECE273_Scribe/scribe_L3.pdf" target=&ldquo;blank&rdquo;>scribe3</a>   </td><td class="c5">         </td></tr>
<tr class="r5"><td class="c1"></td><td class="c2"> Apr 11 </td><td class="c3">   L4: Reduction         </td><td class="c4">   <a href="./ECE273_Scribe/scribe_L4.pdf" target=&ldquo;blank&rdquo;>scribe4</a>     </td><td class="c5">     </td></tr>
<tr class="r6"><td class="c1">3    </td><td class="c2"> Apr 16 </td><td class="c3">   L5: Convex Analysis II </td><td class="c4"> <a href="./ECE273_Scribe/scribe_L5.pdf" target=&ldquo;blank&rdquo;>scribe5</a> </td><td class="c5"> HW1 due; HW2 out </td></tr>
<tr class="r7"><td class="c1"></td><td class="c2"> Apr 18 </td><td class="c3">   L6: Projected Gradient Descent and Frank-Wolfe        </td><td class="c4">    <a href="./ECE273_Scribe/scribe_L6.pdf" target=&ldquo;blank&rdquo;>scribe6</a>      </td><td class="c5">   </td></tr>
<tr class="r8"><td class="c1">4    </td><td class="c2"> Apr 23 </td><td class="c3">   L7: Intoduction to Stochastic Optimization   </td><td class="c4"> <a href="./ECE273_Scribe/scribe_L7.pdf" target=&ldquo;blank&rdquo;>scribe7</a> </td><td class="c5">           </td></tr>
<tr class="r9"><td class="c1"></td><td class="c2"> Apr 25 </td><td class="c3">   L8: SGD variants  </td><td class="c4">  <a href="./ECE273_Scribe/scribe_L8.pdf" target=&ldquo;blank&rdquo;>scribe8</a> </td><td class="c5"> HW2 due; HW3 out </td></tr>
<tr class="r10"><td class="c1">5    </td><td class="c2"> Apr 30 </td><td class="c3">   L9: Duality Theory I: Lagrangian and Weak Duality     </td><td class="c4">  <a href="./ECE273_Scribe/scribe_L9.pdf" target=&ldquo;blank&rdquo;>scribe9</a> </td><td class="c2">           </td></tr>
<tr class="r12"><td class="c1"></td><td class="c2"> May 02 </td><td class="c3">   L10: Duality Theory II: KKT conditions and Strong Duality  </td><td class="c4">  <a href="./ECE273_Scribe/scribe_L10.pdf" target=&ldquo;blank&rdquo;>scribe10</a> </td><td class="c5">  </td></tr>
<tr class="r13"><td class="c1">6    </td><td class="c2"> May 07 </td><td class="c3">   L11: Duality Theory III: Conjugate functions and SDCA    </td><td class="c4"> <a href="./ECE273_Scribe/scribe_L11.pdf" target=&ldquo;blank&rdquo;>scribe11</a> </td><td class="c5">  HW3 due; HW4 out </td></tr>
<tr class="r14"><td class="c1"></td><td class="c2"> May 09 </td><td class="c3">   L12: Mirror Descent                   </td><td class="c4">  <a href="./ECE273_Scribe/scribe_L12.pdf" target=&ldquo;blank&rdquo;>scribe12</a>   </td><td class="c5">       </td></tr>
<tr class="r15"><td class="c1">7    </td><td class="c2"> May 14 </td><td class="c3">   L13: Online Convex Optimization              </td><td class="c4"> <a href="./ECE273_Scribe/scribe_L13.pdf" target=&ldquo;blank&rdquo;>scribe13</a>     </td><td class="c5">   </td></tr>
<tr class="r16"><td class="c1"></td><td class="c2"> May 16 </td><td class="c3">   L14: Online Convex Optimization          </td><td class="c4"> <a href="./ECE273_Scribe/scribe_L14.pdf" target=&ldquo;blank&rdquo;>scribe14</a> </td><td class="c5"> HW4 due; <b> Checkpoint Report Due </b>      </td></tr>
<tr class="r17"><td class="c1">8    </td><td class="c2"> May 21 </td><td class="c3">   L15  Min-Max Optimization </td><td class="c4">    <a href="./ECE273_Scribe/scribe_L15.pdf" target=&ldquo;blank&rdquo;>scribe15</a>    </td><td class="c5">         </td></tr>
<tr class="r18"><td class="c1"></td><td class="c2"> May 23 </td><td class="c3">   <b> Midterm </b>    </td><td class="c4">       </td><td class="c5">     </td></tr>
<tr class="r19"><td class="c1">9    </td><td class="c2"> May 28 </td><td class="c3">   L16: Min-Max Optimization  </td><td class="c4">  <a href="./ECE273_Scribe/scribe_L16.pdf" target=&ldquo;blank&rdquo;>scribe16</a> </td><td class="c5">  HW5 out         </td></tr>
<tr class="r20"><td class="c1"></td><td class="c2"> May 30 </td><td class="c3">   L17: Acceleration via Chebyshev Polynomial   </td><td class="c4">  <a href="./ECE273_Scribe/scribe_L17.pdf" target=&ldquo;blank&rdquo;>scribe17</a>      </td><td class="c5">      </td></tr>
<tr class="r21"><td class="c1">10   </td><td class="c2"> Jun 04 </td><td class="c3">   L18: Optimization as a Game </td><td class="c4">      </td><td class="c5">      </td></tr>
<tr class="r22"><td class="c1"></td><td class="c2"> Jun 06 </td><td class="c3">   L19: Concluding remarks   </td><td class="c4">   </td><td class="c5">        </td></tr>
<tr class="r23"><td class="c1">11   </td><td class="c2"> Jun 10 </td><td class="c3"> Project Presentation        </td><td class="c4">           </td><td class="c5"> <b> Written Report Due </b>    </td></tr>
<tr class="r24"><td class="c1">
</td></tr></table>
<h2>Homework Policy</h2>
<p>Students can work with up to two other classmates on the homeworks. 
However, the solutions submitted must be independently written in your own words.
As a hard rule, you must write up your solution individually and cannot copy your classmates&rsquo; solutions or let your classmates copy your solutions; otherwise, you may not receive any credit for the HW problem(s). You must write your collaborators’ names on the top of your assignment. If you do not work with collaborators, list “Collaborators: None.”
</p>
<p>Students are encouraged to submit a typed solution. Written solutions are allowed but have to be clearly readable. Students might not receive credit for the problems if the written solutions are incomprehensible. We reserve the right to determine if a written solution is incomprehensible.
</p>
<p>Each homework and project assignment will have a clearly stated due date. We expect you to turn in all completed problem sets on time. Late submissions and deadline extensions will not be possible for any reason. 
</p>
<h2>Midterm</h2>
<p>The midterm will take place during regularly scheduled class hours on <b> Thursday, May 23, 2024, from 3:30 pm to 4:50 pm </b>. It will be a closed-book exam; however, students are permitted to use a hand-written cheat sheet, limited to one page and two sides, on standard printer paper size (8.5 x 11 inches). The use of electronic devices, such as phones, tablets, laptops, and earbuds, is not allowed during the exam.
</p>
<p>Students are expected to comply with <a href="https://senate.ucsd.edu/Operating-Procedures/Senate-Manual/Appendices/2" target=&ldquo;blank&rdquo;>UCSD's academic integrity policy</a> throughout the duration of the midterm. Instances of academic dishonesty will be referred to the Office of Student Conduct for adjudication.
</p>
<h2>Final Projects</h2>
<p>Final project: You can form a group of size up to 3 members for the final project. 
</p>
<p><b> Grading of the final project: </b>  1% for a checkpoint report, 12% for a written report, 12 % for an oral presentation.  
</p>
<p><b> Written report of the final project : </b>
</p>
<ul>
<li><p>Describe an optimization paper in detail using your own words. The paper you focus on should be a peer-reviewed paper from top venues such as ICML, NeurIPS, ICLR, COLT, JMLR, Mathematical Programming, or SIAM Journal on Optimization from the past three years. Focusing on papers not from these venues requires discussion with the instructor and obtaining approval first.
</p>
</li>
</ul>
<p>Students are required to <b>submit a typed write-up</b> for the final project consisting of 4-6 pages (the list of references does not count toward the page limit). Submitting a report that is less than 4 full pages (references excluded) will result in a score deduction.  The write-up must contain a significant theoretical component. Please use the Latex template provided in this link <a href="https://media.neurips.cc/Conferences/NeurIPS2024/Styles.zip" target=&ldquo;blank&rdquo;>Latex Template</a>.
</p>
<p>The report should include <b> the motivation, a description of the algorithm, the underlying algorithmic design principles, and a clear reproduction or summary of the theoretical analysis</b>. 
When reproducing the proof, ask yourself: What is the most essential part of the analysis? Can you significantly simplify the proofs without trivializing the results? Because of the page limit, you should avoid presenting every step in the analysis in your written report. Focus on conveying the logical flow and key steps of the theoretical analysis.
</p>
<p>Students are encouraged to perform simple simulations to test the proposed algorithm(s) in the paper
and report their findings in the report, if the paper includes experiments. Code snippets and additional simulation results can be provided in the appendix (after the list of references if any) and do not contribute to the page limit, but the instructor does not have an obligation to read the appendix. The simulations of the method(s) in the paper can be conducted using small, toy, and synthetic data created by your own to showcase the convergent behavior of the algorithm(s) and their baselines. Students do not need to precisely follow the setting nor use the same data as described in the paper. Some papers are purely theoretical without any numerical experiments. In this case, students are still encouraged to create their own synthetic data or come up with a simulation scenario to verify the theoretical results. However, the evaluation of the written report will be focused on <b> whether you can present the motivation, the algorithm, the idea behind the algorithm design, and reproducing/summarizing the analysis clearly </b>. 
Simulations are optional and will only be considered for the assessment if the other aforementioned components of the written report are not strong.  
</p>
<p><b> The due date of the written report is Thursday, Jun 13, 2024, 11:59 pm. </b> 
</p>
<p><b> Additional Tasks (Optional) </b> include (1)
identifying the limitations and proposing a way to overcome them (theoretically and/or empirically), (2) or improving the results in the paper, such as obtaining a tighter bound or modifying the algorithms to be faster (theoretically and/or empirically), (3) or proposing a new algorithm in the same setting. Final projects which include an additional task with a <b> solid </b> result may receive extra credits.
If an additional task is conducted, students should highlight what they have accomplished in the main text of the written report and may provide further detail in the appendix after Page 6, but the instructor does not have an obligation to read the appendix. 
</p>
<p><b> Final presentation for the project: </b>
</p>
<p>The session for in-person oral presentations is scheduled for <b>Monday, Jun 10, 2024, from 15:00 pm to 18:00 pm</b> (the final exam time).
If you do not plan to attend the final presentation in person, please record your video presentation (recommended using Zoom) and upload/share the Zoom link under Canvas/Assignment no later than <b>Thursday, Jun 13, 2024, 11:59 PM</b>. 
Each team has 10 minutes (subject to change) for their presentation and 1 minute (subject to change) for the Q&amp;A.  A team can earn extra credits in their oral presentation by asking a good question or making a constructive remark on the presentations of other teams during the Q&amp;A. 
Similarly, a team can earn extra credits by answering questions well during 
the Q&amp;A. So, you are encouraged to attend the final presentaion.
</p>
<p>Given the limited time available for each oral presentation, each team should avoid walking through the theoretical analysis in the slides presentation. Instead, the oral presentation should focus on giving a clear high-level picture and key messages of the paper, as if you were the authors aiming to deliver a great <b>lightning</b> talk to draw attention from your audience so that they would be interested in reading your paper offline.
</p>
<p>The first couple of minutes/slides should be allocated to describe the setup as clear as possible and/or provide necessary background so that the audience can be on the same page. Subsequently, the main results of the paper should be clearly summarized. If you think you have completed an additional task listed in Additional Tasks above, make sure to allocate at least one-third of time to highlight what you have accomplished.
</p>
<p><b> Checkpoint report of the Final Project: </b>
In the checkpoint report, students are required to include the following information:
</p>
<ul>
<li><p>The name of each team member.
</p>
</li>
<li><p>The title of the paper that you will focus on.
</p>
</li>
</ul>
<p>It is emphasized that every member in the team should submit a checkpoint report. The checkpoint report is due on <b> May 17 Friday at 11:59 PM </b>.
</p>
<h2>Scribe </h2>
<p>Starting from the second lecture, each lecture will have two to three students to scribe. This group of students will be responsible for taking detailed notes in class, and the quality of these notes will be graded. There will be a sign-up sheet for students to sign up to scribe for a preferred date.
</p>
<p>Students should use the provided scribe template available on Canvas
(also available on Overleaf) and email a typed PDF file <b>with</b> the LaTeX code within 6 days to the TA. The TA will review the scribe and may request students to make updates if any part of the notes is unclear or contains errors. It is expected that students revise the scribe accordingly within a couple of days. Students will receive no more than one request if there are issues in the scribe. If students do not address the issues or errors in the scribe timely, then TA may update the scribe; in this case, students may not receive full credit.
</p>
<h2>References</h2>
<p>There is no required textbooks. The course will draw some of the materials from the following references. All the following materials are accessible online or accessible via UCSD library online portal. 
</p>
<ul>
<li><p>Dmitriy Drusvyatskiy <a href="https://sites.math.washington.edu/~ddrusv/crs/Math_516_2021/bookwithindex.pdf" target=&ldquo;blank&rdquo;>Convex Analysis and Nonsmooth Optimization</a>
</p>
</li>
</ul>
<ul>
<li><p>Francesco Orabona <a href="https://arxiv.org/pdf/1912.13213.pdf" target=&ldquo;blank&rdquo;>A Modern Introduction to Online Learning</a>
</p>
</li>
</ul>
<ul>
<li><p>Aaron Sidford <a href="https://drive.google.com/file/d/1BfMkt2glaZpJGwg7gwsJw9T_XxH3o8gx/view" target=&ldquo;blank&rdquo;>Optimization Algorithms</a>
</p>
</li>
</ul>
<ul>
<li><p>Stephen Boyd and Lieven Vandenberghe <a href="https://web.stanford.edu/~boyd/cvxbook/" target=&ldquo;blank&rdquo;>Convex Optimization</a>
</p>
</li>
</ul>
<ul>
<li><p>John Duchi <a href="https://stanford.edu/~jduchi/PCMIConvex/Duchi16.pdf" target=&ldquo;blank&rdquo;>Introductory Lectures on Stochastic Convex Optimization</a>
</p>
</li>
</ul>
<ul>
<li><p>Arkadi Nemirovski <a href="https://www2.isye.gatech.edu/~nemirovs/LMCOLN2022Fall.pdf" target=&ldquo;blank&rdquo;>Lectures on Modern Convex Optimization</a>
</p>
</li>
</ul>
<ul>
<li><p>Nisheeth K. Vishnoi <a href="https://convex-optimization.github.io/" target=&ldquo;blank&rdquo;>Algorithms for Convex Optimization</a>
</p>
</li>
</ul>
<ul>
<li><p>Sebastien Bubeck <a href="https://arxiv.org/pdf/1405.4980.pdf" target=&ldquo;blank&rdquo;>Convex Optimization: Algorithms and Complexity</a>. 
</p>
</li>
</ul>
<ul>
<li><p>Yin Tat Lee and Santosh Vempala <a href="https://github.com/YinTat/optimizationbook/blob/main/main.pdf" target=&ldquo;blank&rdquo;>Techniques in Optimization and Sampling</a>
</p>
</li>
</ul>
<ul>
<li><p>Shai Shalev-Shwartz <a href="https://www.cs.huji.ac.il/~shais/papers/OLsurvey.pdf" target=&ldquo;blank&rdquo;>Online Learning and Online Convex Optimization</a>
</p>
</li>
</ul>
<ul>
<li><p>Elad Hazan <a href="https://arxiv.org/pdf/1909.05207.pdf" target=&ldquo;blank&rdquo;>Introduction to Online Convex Optimization</a>
</p>
</li>
</ul>
<ul>
<li><p>Alexandre d'Aspremont, Damien Scieur, Adrien Taylor <a href="https://arxiv.org/abs/2101.09545.pdf" target=&ldquo;blank&rdquo;>Acceleration Methods</a>
</p>
</li>
</ul>
<ul>
<li><p>Benjamin Recht and Stephen J. Wright <a href="https://people.eecs.berkeley.edu/~brecht/opt4ml_book/" target=&ldquo;blank&rdquo;>Optimization for Modern Data Analysis</a>
</p>
</li>
</ul>
<ul>
<li><p>Guanghui Lan <a href="https://link.springer.com/book/10.1007/978-3-030-39568-1" target=&ldquo;blank&rdquo;>First-order and Stochastic Optimization Methods for Machine Learning</a>
</p>
</li>
</ul>
<ul>
<li><p>Yurii Nesterov <a href="https://link.springer.com/book/10.1007/978-1-4419-8853-9" target=&ldquo;blank&rdquo;>Introductory Lectures on Convex Optimization</a>
</p>
</li>
</ul>
<ul>
<li><p>Amir Beck <a href="https://epubs.siam.org/doi/book/10.1137/1.9781611977622" target=&ldquo;blank&rdquo;>Introduction to Nonlinear Optimization - Theory, Algorithms and Applications</a>
</p>
</li>
</ul>
<h2>Great youtube channels for learning optimization</h2>
<ul>
<li><p>Constantine Caramanis <a href="https://www.youtube.com/playlist?list=PLXsmhnDvpjORzPelSDs0LSDrfJcqyLlZc" target=&ldquo;blank&rdquo;>Optimization Algorithms</a>
</p>
</li>
</ul>
<h2>Acknowledgments</h2>
<p>The design of this course is inspired by the following excellent courses:
</p>
<ul>
<li><p><a href="https://ee227c.github.io/" target=&ldquo;blank&rdquo;>Convex Optimization and Approximation</a> at UC Berkeley by Moritz Hardt. 
</p>
</li>
</ul>
<ul>
<li><p><a href="https://yuxinchen2020.github.io/large_scale_optimization/index.html" target=&ldquo;blank&rdquo;>Large-Scale Optimization for Data Science</a> at University of Pennsylvania by Yuxin Chen. 
</p>
</li>
</ul>
<ul>
<li><p><a href="https://optmlclass.github.io/" target=&ldquo;blank&rdquo;>Optimization for Machine Learning</a> at Boston University by Ashok Cutkosky.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://web.stanford.edu/~sidford/courses/19fa_opt_theory/fa19_opt_theory.html" target=&ldquo;blank&rdquo;>Introduction to Optimization Theory</a> at Standford by Aaron Sidford.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://www.stat.cmu.edu/~ryantibs/convexopt/" target=&ldquo;blank&rdquo;>Convex Optimization</a> at CMU by Ryan Tibshirani.  
</p>
</li>
</ul>
<p>The course webpage and the course policy is inspired by the following excellent courses:
</p>
<ul>
<li><p><a href="https://zhengy09.github.io/ECE285/ece285.html" target=&ldquo;blank&rdquo;>Semidefinite and Sum-of-squares Optimization</a> at UC San Diego by Yang Zheng
</p>
</li>
<li><p><a href="https://mltheory.github.io/CS7545/" target=&ldquo;blank&rdquo;>Machine Learning Theory</a> at Georgia Tech by Jacob Abernethy
</p>
</li>
<li><p><a href="https://nanjiang.cs.illinois.edu/cs542/" target=&ldquo;blank&rdquo;>Statistical Reinforcement Learning</a> at UIUC by Nan Jiang
</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2024-09-11 12:40:14 PDT, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
